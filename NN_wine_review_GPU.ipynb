{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import chainer\n",
    "from chainer import backend\n",
    "from chainer import backends\n",
    "from chainer.backends import cuda\n",
    "from chainer import Function, gradient_check, report, training, utils, Variable\n",
    "from chainer import datasets, initializers, iterators, optimizers, serializers\n",
    "from chainer import Link, Chain, ChainList\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "from chainer.training import extensions\n",
    "import random\n",
    "import cupy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mempool = cupy.get_default_memory_pool()\n",
    "pinned_mempool = cupy.get_default_pinned_memory_pool()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(mempool.used_bytes())  \n",
    "print(mempool.total_bytes()) \n",
    "print(pinned_mempool.n_free_blocks())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning GPU memory\n",
    "mempool.free_all_blocks()\n",
    "pinned_mempool.free_all_blocks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to append all the data and shuffle it. so it would scatter properly.\n",
    "\n",
    "Using the cleaned data which store in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data1.txt', 'r')\n",
    "x1 = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data2.txt', 'r')\n",
    "x2 = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data3.txt', 'r')\n",
    "x3 = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalx = x1+x2+x3\n",
    "random.shuffle(totalx)\n",
    "x = totalx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X now become a list of string as we read from txt file.\n",
    "we need to convert the string into list.\n",
    "\n",
    "[1:29999] is all the input x\n",
    "\n",
    "[30001:30003] is the score. or or [30001:30004] if it is 100(as all scores is between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.524568557739258\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "data1_fin = []\n",
    "start = time.time()\n",
    "for index in range(10000):\n",
    "    temp_data = x[index][1:29999]\n",
    "    if x[index][30003] == '0': #the score is 100\n",
    "        score = int(x[index][30001:30004])\n",
    "    else: #the score is between 80-99\n",
    "        score = int(x[index][30001:30003])\n",
    "    indx = int(x[index][30005:-2])\n",
    "    temp_data1 = temp_data.split(', ')\n",
    "    data = [int(i) for i in temp_data1]\n",
    "    data.append(score)\n",
    "    data.append(indx)\n",
    "    data1_fin.append(data)\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for i in range(len(data1_fin)):\n",
    "    x.append(data1_fin[i][0:-2])\n",
    "    y.append([data1_fin[i][-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = cuda.cupy.array(x, dtype=cupy.float32)\n",
    "x2 = np.array(x, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y1 = cuda.cupy.array(y, dtype=cupy.float32)\n",
    "y2 = np.array(y, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(y2[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_data = len(x)\n",
    "# variance_of_noise = 0.12\n",
    "train_ratio = 0.8 #how much data will be used in training\n",
    "slice_position = int(number_of_data * train_ratio)\n",
    "x_train = x1[:slice_position]\n",
    "y_train = y1[:slice_position]\n",
    "x_train = Variable(x_train)\n",
    "y_train = Variable(y_train)\n",
    "\n",
    "x_test = x1[slice_position:]\n",
    "y_test = y1[slice_position:]\n",
    "x_test = Variable(x_test)\n",
    "y_test = Variable(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train2 = x2[:slice_position]\n",
    "y_train2 = y2[:slice_position]\n",
    "x_train2 = Variable(x_train2)\n",
    "y_train2 = Variable(y_train2)\n",
    "\n",
    "x_test2 = x2[slice_position:]\n",
    "y_test2 = y2[slice_position:]\n",
    "x_test2 = Variable(x_test2)\n",
    "y_test2 = Variable(y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 12.182865\n",
      "epoch: 1 loss: 10.881067\n",
      "epoch: 2 loss: 5.102632\n",
      "epoch: 3 loss: 3.8800995\n",
      "epoch: 4 loss: 3.3558893\n",
      "epoch: 5 loss: 3.0711718\n",
      "epoch: 6 loss: 2.8516386\n",
      "epoch: 7 loss: 2.713988\n",
      "epoch: 8 loss: 2.568303\n",
      "epoch: 9 loss: 2.4890976\n",
      "loss of test data: 4.3794103\n",
      "used time:  17.14109468460083\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from chainer import Chain, Variable,optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "start = time.time()\n",
    "#### Defining a class for our NN model\n",
    "class Regression(Chain):\n",
    "    def __init__(self):\n",
    "        super(Regression,self).__init__(\n",
    "            ### One hidden layer\n",
    "            l1 = L.Linear(10000,4000),\n",
    "            l2 = L.Linear(4000,1),\n",
    "    )\n",
    "\n",
    "    def __call__(self,x):\n",
    "        ### We define non-linear function here\n",
    "        ### We use relu but I suggest to use sigmoid just because I like it!\n",
    "        h = F.relu(self.l1(x)) \n",
    "        h = self.l2(h)\n",
    "        return h\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Parameters\n",
    "    batchsize = 100\n",
    "    max_epoch = 10\n",
    "        \n",
    "    model = Regression()\n",
    "    model.to_gpu(0)\n",
    "    optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    #Train\n",
    "    N = len(x_train)\n",
    "    perm = cupy.random.permutation(N)\n",
    "    for epoch in range(max_epoch):\n",
    "        for i in range(0,N,batchsize):\n",
    "            x_train_batch = x_train[perm[i:i + batchsize]]\n",
    "            y_train_batch = y_train[perm[i:i + batchsize]]\n",
    "\n",
    "            ## Before calculating GD, we need to clear the gradients.\n",
    "            model.cleargrads()\n",
    "            loss = F.mean_squared_error(model(x_train_batch),y_train_batch)\n",
    "            ### I beleive this one below calculate backward, which means calculating gradient for each parameter\n",
    "            loss.backward()\n",
    "            ### Now, using the gradient calculated above, they update the parameter\n",
    "            optimizer.update()\n",
    "            \n",
    "        print(\"epoch:\",epoch,\"loss:\",loss.data)\n",
    "\n",
    "#Test\n",
    "    model.cleargrads()\n",
    "    t_test = model(x_test)\n",
    "    loss = F.mean_squared_error(t_test,y_test)\n",
    "    print(\"loss of test data:\",loss.data)\n",
    "\n",
    "    #x_grandtruth = np.arange(-1.7,1.7,0.1)\n",
    "    #y_grandtruth = x_grandtruth**4-2*x_grandtruth**2+0.5*x_grandtruth+2.0\n",
    "\n",
    "    #Plot1\n",
    "    # plt.plot(x_grandtruth,y_grandtruth,color=\"red\",label=\"Grand Truth\")\n",
    "    \n",
    "end=time.time()\n",
    "print(\"used time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 13.460535\n",
      "epoch: 1 loss: 10.387961\n",
      "epoch: 2 loss: 5.5507107\n",
      "epoch: 3 loss: 3.5093808\n",
      "epoch: 4 loss: 3.1015766\n",
      "epoch: 5 loss: 2.9695795\n",
      "epoch: 6 loss: 2.872941\n",
      "epoch: 7 loss: 2.8172054\n",
      "epoch: 8 loss: 2.7580001\n",
      "epoch: 9 loss: 2.7018926\n",
      "loss of test data: 3.967285\n",
      "used time:  436.88626527786255\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#### Defining a class for our NN model\n",
    "class Regression(Chain):\n",
    "    def __init__(self):\n",
    "        super(Regression,self).__init__(\n",
    "            ### One hidden layer\n",
    "            l1 = L.Linear(10000,4000),\n",
    "            l2 = L.Linear(4000,1),\n",
    "    )\n",
    "\n",
    "    def __call__(self,x):\n",
    "        ### We define non-linear function here\n",
    "        ### We use relu but I suggest to use sigmoid just because I like it!\n",
    "        h = F.relu(self.l1(x))\n",
    "        h = self.l2(h)\n",
    "        return h\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Parameters\n",
    "    batchsize = 100\n",
    "    max_epoch = 10\n",
    "        \n",
    "    model2 = Regression()\n",
    "    optimizer = optimizers.MomentumSGD(lr=0.01, momentum=0.9)\n",
    "    optimizer.setup(model2)\n",
    "\n",
    "    #Train\n",
    "    N = len(x_train2)\n",
    "    perm = np.random.permutation(N)\n",
    "    for epoch in range(max_epoch):\n",
    "        for i in range(0,N,batchsize):\n",
    "            x_train_batch2 = x_train2[perm[i:i + batchsize]]\n",
    "            y_train_batch2 = y_train2[perm[i:i + batchsize]]\n",
    "\n",
    "            ## Before calculating GD, we need to clear the gradients.\n",
    "            model2.cleargrads()\n",
    "            loss = F.mean_squared_error(model2(x_train_batch2),y_train_batch2)\n",
    "            ### I beleive this one below calculate backward, which means calculating gradient for each parameter\n",
    "            loss.backward()\n",
    "            ### Now, using the gradient calculated above, they update the parameter\n",
    "            optimizer.update()\n",
    "            \n",
    "        print(\"epoch:\",epoch,\"loss:\",loss.data)\n",
    "\n",
    "#Test\n",
    "    model2.cleargrads()\n",
    "    t_test2 = model2(x_test2)\n",
    "    loss = F.mean_squared_error(t_test2,y_test2)\n",
    "    print(\"loss of test data:\",loss.data)\n",
    "\n",
    "    #x_grandtruth = np.arange(-1.7,1.7,0.1)\n",
    "    #y_grandtruth = x_grandtruth**4-2*x_grandtruth**2+0.5*x_grandtruth+2.0\n",
    "\n",
    "    #Plot1\n",
    "    # plt.plot(x_grandtruth,y_grandtruth,color=\"red\",label=\"Grand Truth\")\n",
    "    \n",
    "end=time.time()\n",
    "print(\"used time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU availability: True\n",
      "cuDNN availablility: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU availability:', chainer.cuda.available)\n",
    "print('cuDNN availablility:', chainer.cuda.cudnn_enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_batch2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
