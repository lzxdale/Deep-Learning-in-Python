{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linzexiang/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from chainer import Chain, Variable,optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We need to append all the data and shuffle it. so it would scatter properly.\n",
    "\n",
    "Using the cleaned data which store in advance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data1.txt', 'r')\n",
    "x1 = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data2.txt', 'r')\n",
    "x2 = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('Data3.txt', 'r')\n",
    "x3 = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "totalx = x1+x2+x3\n",
    "random.shuffle(totalx)\n",
    "x = totalx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### X now become a list of string as we read from txt file.\n",
    "we need to convert the string into list.\n",
    "\n",
    "[1:29999] is all the input x\n",
    "\n",
    "[30001:30003] is the score. or or [30001:30004] if it is 100(as all scores is between 80-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.319114923477173\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "data1_fin = []\n",
    "start = time.time()\n",
    "for index in range(10000):\n",
    "    temp_data = x[index][1:29999]\n",
    "    if x[index][30003] == '0': #the score is 100\n",
    "        score = int(x[index][30001:30004])\n",
    "    else: #the score is between 80-99\n",
    "        score = int(x[index][30001:30003])\n",
    "    indx = int(x[index][30005:-2])\n",
    "    temp_data1 = temp_data.split(', ')\n",
    "    data = [int(i) for i in temp_data1]\n",
    "    data.append(score)\n",
    "    data.append(indx)\n",
    "    data1_fin.append(data)\n",
    "\n",
    "end=time.time()\n",
    "print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = list()\n",
    "y = list()\n",
    "\n",
    "for i in range(len(data1_fin)):\n",
    "    x.append(data1_fin[i][0:-2])\n",
    "    y.append([data1_fin[i][-2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_data = len(x)\n",
    "# variance_of_noise = 0.12\n",
    "train_ratio = 0.8 #how much data will be used in training\n",
    "slice_position = int(number_of_data * train_ratio)\n",
    "x_train = x[:slice_position]\n",
    "y_train = y[:slice_position]\n",
    "x_train = Variable(np.array(x_train,dtype=np.float32))\n",
    "y_train = Variable(np.array(y_train,dtype=np.float32))\n",
    "\n",
    "x_test = x[slice_position:]\n",
    "y_test = y[slice_position:]\n",
    "x_test = Variable(np.array(x_test,dtype=np.float32))\n",
    "y_test = Variable(np.array(y_test,dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0 loss: 12.778955\n",
      "epoch: 1 loss: 9.97185\n",
      "epoch: 2 loss: 9.988987\n",
      "epoch: 3 loss: 9.989281\n",
      "epoch: 4 loss: 9.675341\n",
      "epoch: 5 loss: 4.1746864\n",
      "epoch: 6 loss: 3.3703365\n",
      "epoch: 7 loss: 3.1048756\n",
      "epoch: 8 loss: 3.0345592\n",
      "epoch: 9 loss: 3.290944\n",
      "loss of test data: 4.169567\n",
      "used time:  603.5054240226746\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from chainer import Chain, Variable,optimizers\n",
    "import chainer.functions as F\n",
    "import chainer.links as L\n",
    "\n",
    "start = time.time()\n",
    "#### Defining a class for our NN model\n",
    "class Regression(Chain):\n",
    "    def __init__(self):\n",
    "        super(Regression,self).__init__(\n",
    "            ### One hidden layer\n",
    "            l1 = L.Linear(10000,4000),\n",
    "            l2 = L.Linear(4000,1),\n",
    "    )\n",
    "\n",
    "    def __call__(self,x):\n",
    "        ### We define non-linear function here\n",
    "        ### We use relu but I suggest to use sigmoid just because I like it!\n",
    "        h = F.relu(self.l1(x))\n",
    "        h = self.l2(h)\n",
    "        return h\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #Parameters\n",
    "    batchsize = 100\n",
    "    max_epoch = 10\n",
    "        \n",
    "    model = Regression()\n",
    "    optimizer = optimizers.MomentumSGD(lr=0.01,momentum=0.9)\n",
    "    optimizer.setup(model)\n",
    "\n",
    "    #Train\n",
    "    N = len(x_train)\n",
    "    perm = np.random.permutation(N)\n",
    "    for epoch in range(max_epoch):\n",
    "        for i in range(0,N,batchsize):\n",
    "            x_train_batch = x_train[perm[i:i + batchsize]]\n",
    "            y_train_batch = y_train[perm[i:i + batchsize]]\n",
    "\n",
    "            ## Before calculating GD, we need to clear the gradients.\n",
    "            model.cleargrads()\n",
    "            loss = F.mean_squared_error(model(x_train_batch),y_train_batch)\n",
    "            ### I beleive this one below calculate backward, which means calculating gradient for each parameter\n",
    "            loss.backward()\n",
    "            ### Now, using the gradient calculated above, they update the parameter\n",
    "            optimizer.update()\n",
    "            \n",
    "        print(\"epoch:\",epoch,\"loss:\",loss.data)\n",
    "\n",
    "#Test\n",
    "    model.cleargrads()\n",
    "    t_test = model(x_test)\n",
    "    loss = F.mean_squared_error(t_test,y_test)\n",
    "    print(\"loss of test data:\",loss.data)\n",
    "\n",
    "    #x_grandtruth = np.arange(-1.7,1.7,0.1)\n",
    "    #y_grandtruth = x_grandtruth**4-2*x_grandtruth**2+0.5*x_grandtruth+2.0\n",
    "\n",
    "    #Plot1\n",
    "    # plt.plot(x_grandtruth,y_grandtruth,color=\"red\",label=\"Grand Truth\")\n",
    "    \n",
    "end=time.time()\n",
    "print(\"used time: \", end-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
